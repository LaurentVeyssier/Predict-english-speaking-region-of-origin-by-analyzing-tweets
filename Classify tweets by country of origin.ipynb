{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify tweets by predicting country of origin using feature extraction from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import cytoolz as ct\n",
    "from gensim.parsing import preprocessing\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(\"srv\", \"data\", \"shared_data_folder\", \"data\", \"Twitter_by_Country.gz\")\n",
    "df  = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Country</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Australia</td>\n",
       "      <td>this long thread on medieval dyes and pigments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Australia</td>\n",
       "      <td>no i didnt just bust my ass to she in the dark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Australia</td>\n",
       "      <td>give him a beak he s just speaking in tongues ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Australia</td>\n",
       "      <td>great movie great ending to years of my lifeun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Australia</td>\n",
       "      <td>happy holidays we wish you all the best for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140243</th>\n",
       "      <td>1140243</td>\n",
       "      <td>United_States</td>\n",
       "      <td>you also decided that i didn t read the articl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140244</th>\n",
       "      <td>1140244</td>\n",
       "      <td>United_States</td>\n",
       "      <td>great info do you know what to do participate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140245</th>\n",
       "      <td>1140245</td>\n",
       "      <td>United_States</td>\n",
       "      <td>my yo gave me a bad yelp review so i put him u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140246</th>\n",
       "      <td>1140246</td>\n",
       "      <td>United_States</td>\n",
       "      <td>this is real smart city stuff not fake news sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140247</th>\n",
       "      <td>1140247</td>\n",
       "      <td>United_States</td>\n",
       "      <td>tell me why everyone in my sociology groupme c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1140248 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0        Country  \\\n",
       "0                 0      Australia   \n",
       "1                 1      Australia   \n",
       "2                 2      Australia   \n",
       "3                 3      Australia   \n",
       "4                 4      Australia   \n",
       "...             ...            ...   \n",
       "1140243     1140243  United_States   \n",
       "1140244     1140244  United_States   \n",
       "1140245     1140245  United_States   \n",
       "1140246     1140246  United_States   \n",
       "1140247     1140247  United_States   \n",
       "\n",
       "                                                      Text  \n",
       "0        this long thread on medieval dyes and pigments...  \n",
       "1        no i didnt just bust my ass to she in the dark...  \n",
       "2        give him a beak he s just speaking in tongues ...  \n",
       "3        great movie great ending to years of my lifeun...  \n",
       "4        happy holidays we wish you all the best for th...  \n",
       "...                                                    ...  \n",
       "1140243  you also decided that i didn t read the articl...  \n",
       "1140244  great info do you know what to do participate ...  \n",
       "1140245  my yo gave me a bad yelp review so i put him u...  \n",
       "1140246  this is real smart city stuff not fake news sm...  \n",
       "1140247  tell me why everyone in my sociology groupme c...  \n",
       "\n",
       "[1140248 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Country', 'Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Australia</td>\n",
       "      <td>this long thread on medieval dyes and pigments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Australia</td>\n",
       "      <td>no i didnt just bust my ass to she in the dark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Australia</td>\n",
       "      <td>give him a beak he s just speaking in tongues ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Australia</td>\n",
       "      <td>great movie great ending to years of my lifeun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Australia</td>\n",
       "      <td>happy holidays we wish you all the best for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140243</th>\n",
       "      <td>United_States</td>\n",
       "      <td>you also decided that i didn t read the articl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140244</th>\n",
       "      <td>United_States</td>\n",
       "      <td>great info do you know what to do participate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140245</th>\n",
       "      <td>United_States</td>\n",
       "      <td>my yo gave me a bad yelp review so i put him u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140246</th>\n",
       "      <td>United_States</td>\n",
       "      <td>this is real smart city stuff not fake news sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140247</th>\n",
       "      <td>United_States</td>\n",
       "      <td>tell me why everyone in my sociology groupme c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1140248 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Country                                               Text\n",
       "0            Australia  this long thread on medieval dyes and pigments...\n",
       "1            Australia  no i didnt just bust my ass to she in the dark...\n",
       "2            Australia  give him a beak he s just speaking in tongues ...\n",
       "3            Australia  great movie great ending to years of my lifeun...\n",
       "4            Australia  happy holidays we wish you all the best for th...\n",
       "...                ...                                                ...\n",
       "1140243  United_States  you also decided that i didn t read the articl...\n",
       "1140244  United_States  great info do you know what to do participate ...\n",
       "1140245  United_States  my yo gave me a bad yelp review so i put him u...\n",
       "1140246  United_States  this is real smart city stuff not fake news sm...\n",
       "1140247  United_States  tell me why everyone in my sociology groupme c...\n",
       "\n",
       "[1140248 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets:  1140248\n",
      "number of tweets per country:\n",
      "Australia                 192307\n",
      "Ireland                   192307\n",
      "New_Zealand               178713\n",
      "South_Africa              192307\n",
      "United_Kingdom            192307\n",
      "United_States             192307\n"
     ]
    }
   ],
   "source": [
    "print('Total number of tweets: ',len(df))\n",
    "print('number of tweets per country:')\n",
    "for country, df_country in df.groupby('Country'):\n",
    "    print(f'{country:{25}} {len(df_country)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(df.Text, df.Country, test_size=0.1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29421      going to the movies by myself is my preference...\n",
       "751212     when it comes to perfomance you will not exper...\n",
       "839886     thank you for sharing our post have a great da...\n",
       "167733     insert whatever this is epic pepe fuentes styl...\n",
       "380807     plaza de las flores a square lined with floris...\n",
       "                                 ...                        \n",
       "906105     i seem to have missed a lot of madness todayto...\n",
       "1055068    if the guy can throw anything like sales slide...\n",
       "216052     ahhhhb i would say thankyou again also thx for...\n",
       "696442     more spiritual cause i would sleep after jumma...\n",
       "953238     famous dex speaks on juice wrld s passing atoh...\n",
       "Name: Text, Length: 1026223, dtype: object"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess dataset for features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases=None\n",
    "def clean(line, tag = False):\n",
    "    #Remove links, hashtags, at-mentions, mark-up, and \"RT\"\n",
    "    line = re.sub(r\"http\\S+\", \"\", line)\n",
    "    line = re.sub(r\"@\\S+\", \"\", line)\n",
    "    line = re.sub(r\"#\\S+\", \"\", line)\n",
    "    line = re.sub(\"<[^>]*>\", \"\", line)\n",
    "    line = line.replace(\" RT\", \"\").replace(\"RT \", \"\")\n",
    "\n",
    "    #Remove punctuation and extra spaces\n",
    "    line = ct.pipe(line, preprocessing.strip_tags, \n",
    "                   preprocessing.strip_punctuation, \n",
    "                   preprocessing.strip_numeric, \n",
    "                   preprocessing.strip_non_alphanum, \n",
    "                   preprocessing.strip_multiple_whitespaces)\n",
    "\n",
    "    #Strip and lowercase\n",
    "    line = line.lower().strip().lstrip().split()\n",
    "\n",
    "    #If we've used PMI to find phrases, get those phrases now\n",
    "    if phrases != None:\n",
    "        line = list(phrases[line])\n",
    "\n",
    "    #If we want Part-of-Speech tagging, do that now\n",
    "    if tag == True:\n",
    "        line = nlp(\" \".join(line))\n",
    "        line = [w.text + \"_\" + w.pos_ for w in line]\n",
    "        \n",
    "    line = ' '.join([w for w in line])\n",
    "\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this long thread on medieval dyes and pigments is one of the things i love about twitter an expert sharing her knoa couple of years ago we did a quiz so if you have some spare time why not see how much you know about theharry styles fine line cd giveaway rt and reply with your top songs from fine line a picture of harry mbf illa glimmer of hope for the last wild population the cape arid np bushfire while still uncontrolled aa tiny step forward in the battle to save the last wild population cape arid fire doesn t seem to haafter years maybe finally naka read ko usab sa church and it feels so good thank you lord happy birthdaymary godwin writes in her journal in the evening hogg comes i like him better each time it is a pity thaenjoying my extra cold castle lite dankolow key true rocky stopped making decent music after yams diedcrabb robinson a call on blake i read him wordsworth s incomparable ode which he heartily enjoyedsevern writes the continued stretch of keats imagination has killed him amp were he to recover he could notwordsworth and dorothy arrive at dove cottage which they have rented for a year one chimney smokes and twhy the hell do they always give this guy a ball when he s only made like threes total in his careercat i dont care you are my mf friend okay i want to experience this concert as much as you if ii will cover what you cant and if your parents are worried we can make a strict plan with them we la couple of years ago we did a quiz so if you have some spare time why not see how much you know abouti do lament he ever left england the journey of miles was too much i have thought he would die before he repleasepleaseplease camp out with me for harry we ll get a tent and shizand here s giuseppe albano curator of the house on keats last days in rometomorrow kids just remember get what you get and don t get upsetjust received an update from with favourable winds firefighters will attempt to construct a containmentthe wa fire threatening one of australia s rarest birds with extinction is for some reason not mentionedi do lament he ever left england the journey of miles was too much i have thought he would die before hsevern writes the continued stretch of keats imagination has killed him amp were he to recover he couldfire patricia asap team is going and has been going backwards since he took over his demeanor on thi cant vibe with your sis i wanna be crying on a bedroom floor listening to falling and fine line stfu'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0].Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this long thread on medieval dyes and pigments is one of the things i love about twitter an expert sharing her knoa couple of years ago we did a quiz so if you have some spare time why not see how much you know about theharry styles fine line cd giveaway rt and reply with your top songs from fine line a picture of harry mbf illa glimmer of hope for the last wild population the cape arid np bushfire while still uncontrolled aa tiny step forward in the battle to save the last wild population cape arid fire doesn t seem to haafter years maybe finally naka read ko usab sa church and it feels so good thank you lord happy birthdaymary godwin writes in her journal in the evening hogg comes i like him better each time it is a pity thaenjoying my extra cold castle lite dankolow key true rocky stopped making decent music after yams diedcrabb robinson a call on blake i read him wordsworth s incomparable ode which he heartily enjoyedsevern writes the continued stretch of keats imagination has killed him amp were he to recover he could notwordsworth and dorothy arrive at dove cottage which they have rented for a year one chimney smokes and twhy the hell do they always give this guy a ball when he s only made like threes total in his careercat i dont care you are my mf friend okay i want to experience this concert as much as you if ii will cover what you cant and if your parents are worried we can make a strict plan with them we la couple of years ago we did a quiz so if you have some spare time why not see how much you know abouti do lament he ever left england the journey of miles was too much i have thought he would die before he repleasepleaseplease camp out with me for harry we ll get a tent and shizand here s giuseppe albano curator of the house on keats last days in rometomorrow kids just remember get what you get and don t get upsetjust received an update from with favourable winds firefighters will attempt to construct a containmentthe wa fire threatening one of australia s rarest birds with extinction is for some reason not mentionedi do lament he ever left england the journey of miles was too much i have thought he would die before hsevern writes the continued stretch of keats imagination has killed him amp were he to recover he couldfire patricia asap team is going and has been going backwards since he took over his demeanor on thi cant vibe with your sis i wanna be crying on a bedroom floor listening to falling and fine line stfu'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(df.iloc[0].Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(input = \"content\", encoding = \"utf-8\", decode_error = \"replace\", ngram_range = (1, 1), norm = \"l2\", use_idf = True, smooth_idf = True, preprocessor= clean, tokenizer = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(decode_error='replace',\n",
       "                preprocessor=<function clean at 0x000001B1F82A5828>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  10635190\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size: ', len(vectorizer.vocabulary_) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorizer.transform(xtrain)\n",
    "x_test = vectorizer.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of matrix vectors on train set: (1026223, 10635190)\n"
     ]
    }
   ],
   "source": [
    "print('shape of matrix vectors on train set:', x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC(penalty = \"l2\", loss = \"squared_hinge\", dual = True, tol = 0.0001, C = 1.0, multi_class = \"ovr\", fit_intercept = True, intercept_scaling = 1, max_iter = 200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(max_iter=200000)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "     Australia       0.97      0.95      0.96     19215\n",
      "       Ireland       0.98      0.97      0.97     19634\n",
      "   New_Zealand       0.97      0.94      0.96     17696\n",
      "  South_Africa       0.99      0.98      0.98     19032\n",
      "United_Kingdom       0.94      0.97      0.95     19174\n",
      " United_States       0.95      0.99      0.97     19274\n",
      "\n",
      "      accuracy                           0.96    114025\n",
      "     macro avg       0.97      0.96      0.96    114025\n",
      "  weighted avg       0.97      0.96      0.96    114025\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)\n",
    "print(classification_report(ytest, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== Prediction for tweet n°279160 ===============================\n",
      "what reaction from his teammates i wondercracks me upfunny thought provoking strange intelligent and hilarious thanks amp brilliant showi hear kevin and stuart went professional afteri can t find the exact one i bought but these are the same i think you ll like d poover people banded together to recreate one of ireland s most famous mythological feats viathanks yvette i think everyone got stuck in to givewhen i hear euphoria i m fine epiphany i cry my eyes outin fairness billy don t we don t we deserve a govt that is honest productive free of corruption and representathe weirdest thing about was that facebook was still working but other apps internet wouldn t load at allits been days since the conceand i still cry when i hear a song see a concephoto i m legithappy birthday jiminnie you looked like an actual king at the concert getwhat it must be like playing upfront for irelandand in some countries men are not allowed to work at alleveryone hates med and pre med students for being cutthroat and competitive but the medical education system requiris eir s infrastructure managed in ireland or is this an rbs mainframe style thing oh and if the dns servers are dhow do you know if you don t try i believe in you fella probably best if you justshe is a professional dancer who was a judge on another dancing showchairperson ann marie o sullivan launches the annual cork friends of crumlin annual dinner bid online toas i had brunch at the idaho café today run by amp i reflected again on just how important itso so happy with life right now what s for you will never ever pass youwe are looking to hire a project analyst to support the development of the ucc project centre the role offers the opportuni love the fact all my dad s side of the family tell people i m the quiet oneheartbreaking stories from those with first hand experience of tonight a huge thanks to all the donors and spwatching is soul destroying stuff for the love of god when will it end roll on the real footballlive now over on variety stream incoming golf with friends cards against humanity etc etc come join inokay daisy and poppy are my nieces names but whatever\n",
      "\n",
      "True label --------> Ireland\n",
      "Predicted label ---> Ireland\n"
     ]
    }
   ],
   "source": [
    "index = random.sample(list(xtest.index), k=1)[0]\n",
    "print(f'====================== Prediction for tweet n°{index} ===============================')\n",
    "print(xtest[index])\n",
    "print()\n",
    "print(f'True label --------> {ytest[index]}')\n",
    "print(f'Predicted label ---> {model.predict(vectorizer.transform([xtest[index]]))[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
